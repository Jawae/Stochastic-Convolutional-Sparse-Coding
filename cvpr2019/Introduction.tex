\section{Introduction}
Convolutional Sparse Coding (CSC) is a method for learning {\em
  generative} models in the form of translationally invariant
dictionaries for a large variety of different training signals.  These
generative models have been shown effective for solving problems in
neural and brain information
processing~\cite{jas2017learning,peter2017sparse}, as well as in a
variety of image processing tasks, for instance, image
inpainting~\cite{heide2015fast},
super-resolution~\cite{gu2015convolutional}, high dynamic range
imaging~\cite{serrano2016convolutional}, and high-dimensional signal
reconstructions~\cite{choudhury2017consensus,bibi2017high}. CSC
differs from conventional sparse coding by formulating the signals as
the sum of a set of convolutions on dictionary filters and sparse
codes instead of patch-wise linear combinations of
filters. In traditional sparse dictionary learning, the patch
structure significantly degrades the expressiveness of the
dictionaries by introducing a strong dependency on the position of a
feature, which the convolutional nature of CSC avoids.
%% Alternatively, sparse coding, as a patch-based approach,              %%
%% learns filters from partitioned local structures, and this partition  %%
%% manipulation discards inherent correlations between those patches, so %%
%% as to learns redundant filters (the same or similar ones with         %%
%% translated versions). Owing to the convolutional property, the        %%
%% dictionary learned from CSC achieves signal global coherence, tending %%
%% to be more representative.                                            %%

Of course this convolutional approach is also a the heart of many deep
learning-based methods in the form of
CNNs~\cite{lecun1998gradient,kavukcuoglu2010learning,krizhevsky2012imagenet},
which have in recent years been extraordinarily successful for a broad
range of high-level image understanding applications. However, while
CNNs generally are used in a {\em supervised} setting and produce {\em
  discriminative}, task-specific models, CSC is {\em unsupervised} and
produces {\em generative} models that easily transfer between tasks.

To solve the optimization problems inherent to CSC, Zeiler et
al.~\cite{zeiler2010deconvolutional} iteratively solve two subproblems
(updating sparse codes and updating filters) using gradient decent in
the form of convolutional operations in the spatial domain, which is
computationally expensive. Recent algorithms tackle the problem by
exploiting Parseval's theorem to express the spatial convolution by
multiplication in the frequency domain and using proximal solver such
as Alternating Direction Method of Multipliers
(ADMM)~\cite{boyd2011distributed} to separate the linear least squares
parts terms from the non-smooth terms in the optimization
problem. These approaches show tremendous improvements over prior
spatial-domain solvers with respect to running
time~\cite{bristow2013fast,heide2015fast,wohlberg2016efficient,choudhury2017consensus}. Most
of the prior work learns the dictionary filters in a batch mode, which
indicates that all training signals are involved in every training
iteration, and this restricts it from applying to large datasets or
streaming data.

In contrast to batch mode learning, online
learning~\cite{shalev2012online} is a well established strategy which
processes a single image or a small portion (mini-batch) of the whole
data at each training step, and incrementally updates model
variables. Herein, the required memory and computing sources are only
dependent on the sample size in every observation, independent of the
training data size. It alleviates the scalability issue that arises in
batch approaches, and the convergence of the algorithm was firstly
analyzed using stochastic approximation
tools~\cite{bottou1998online}. Bottou et
al.~\cite{bousquet2008tradeoffs} further showed better generalization
performance of the stochastic algorithms than standard gradient
descent on large scale learning systems. Later on, online learning
strategies were synergetic with sparse coding, which was then scaled
up for learning dictionary from millions of training
samples~\cite{mairal2009online,mairal2010online}, and for large-scale
matrix factorization with an additionally introduced subsampling
strategy~\cite{mensch2016dictionary}. More recently, Liu et
al.~\cite{liu-2018-first} and Wang et al.~\cite{wang2018scalable}
separately proposed similar online learning frameworks for the CSC
model, alleviating the memory issues arise in batch-based CSC model on
large datasets.

{\bfseries Contributions.} We mainly make three contributions in this
work. First, we introduce a randomization strategy for the CSC
model and solve the entire problems in spatial domain. We demonstrate
that the proposed stochastic spatial-domain solver, with a reasonably
selected subsampling rate, outperforms the state-of-the-art
frequency-domain solvers with regard to computing efficiency. We then
formulate an online-learning version of the proposed algorithm, and 
show dramatic runtime improvement over current online CSC methods,
while producing comparable outcomes. Finally, we demonstrate the
capability to learn the over-complete dictionary from thousands of
images, and analyze the effectiveness of the learned over-complete
dictionary for a number of reconstruction tasks.


% --- DO NOT DELETE ---
% Local Variables:
% mode: latex
% mode: flyspell
% mode: TeX-PDF
% End:

