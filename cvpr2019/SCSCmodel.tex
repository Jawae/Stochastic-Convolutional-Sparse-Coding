\section{Stochastic Convolutional Sparse Coding}
\subsection{The Model}
Based on the specific sparsity property of the codes, we propose a variant of the CSC model using subsampling strategy,
and formulate the following modified minimization problem:
\begin{align}
    &\minimize{\filter,\code}  \frac{1}{2}\|\signal - \sum_{k=1}^{K} \filter_k * \mathcal{M}_k(\code_k) \|_2^2 + \lambda \sum_{k=1}^{K}\| \mathcal{M}_k(\code_k) \|_1  \nonumber \\
    &\text{subject to} ~ \|\filter_k\|^2_2 \leq 1 ~~ \forall k \in \{1,\dots,K\},
\end{align}
where $\mathcal{M}_k(\code_k)$ is the operation to subsample the corresponding code $\code_k$ following a Bernoulli distribution with probability $p$. The probability $p$ controls the fraction of the sparse codes that will be considered by the solver, and in the case of $p=1$, the proposed model is identical with the classical CSC model. If $p<1$, the algorithm only solves the sparse codes at chosen positions in each iteration, and accordingly, the update of dictionary $d$ is based on the selected portion of the sparse codes. Similar to solving the classical CSC problem, we can apply coordinate descent algorithm, alternating on subproblems of $\mathcal{M}(\code)$ and $\filter$, to solve the above optimization problem. Specifically, the modified minimization problem for $\code$ can be formulated as:
\begin{equation} \label{eq:updatingCode}
    \code_{opt} = \minimize{\code} \frac{1}{2}\|\signal - (\Filter \mask^\top)(\mask \code) \|_2^2 + \lambda \|\mask \code\|_1,
\end{equation}
where $\Filter = [\Filter_1, \dots, \Filter_K] \in \mathbb{R}^{D \times DK}$, $\code = [\code_1, \dots, \code_K] \in \mathbb{R}^{DK}$, the convolution operator are formulated in matrix multiplication so that $ \Filter \code = \sum_{k=1}^{K} \filter_k * \code_k$.  Matrix $\mask$ is a $\mathbb{R}^{pDk \times DK}$ random diagonal matrix with ones on the entries corresponding to the sampled sparse codes and zeros elsewhere, performing the function of $\mathcal{M}(\code)$. Due to the introduced subsampling matrix, the convolution operator will not hold, hence it cannot be directly solved in the frequency domain. Owing to the subsampling strategy, the number of variables that needs to be computed for this subproblem is $pDK$ instead of $DK$, and this can lead to a reduction of the computation time up to a factor of $\frac{1}{p}$ when solving it in spatial domain.

After obtaining the subsampled sparse codes, we can then project them onto its original spatial support with $0$ filling. Afterwards, the dictionary can be updated by:
\begin{equation} \label{eq:updatingFilter}
\begin{split}
   & \filter_{opt} = \minimize{\filter} \frac{1}{2}\|\signal - \Code \filter \|_2^2 \\
   & \text{subject to}  ~ \|\filter_k\|^2_2 \leq 1 ~ \forall k \in \{1,\dots,K\},
\end{split}
\end{equation}
where $\Code= [\Code_1, \dots, \Code_K] \in \mathbb{R}^{D \times MK}$ is a concatenated matrix, and $\Code_k$ is constructed from the associated $\code_k$, $\filter= [\filter_1,\dots,\filter_K] \in \mathbb{R}^{MK}$ such that $ \Code \filter = \sum_{k=1}^{K} \filter_k * \code_k$. In the common settings for CSC problem, $M \ll D$, hence there is no need to perform subsampling on dictionary.

The work of~\cite{mensch2016dictionary} also applies the stochastic subsampling strategy for dictionary learning, while the subsampling is randomly performed on the observed data, differing from that of our proposed. One can think of our proposed randomization approach as a variant of the stochastic Frank-Wolfe algorithm~\cite{reddi2016stochastic,pmlr-v80-kerdreux18a}, in which a subset of the variables are randomly extracted based on a certain probability distribution at each iteration. For the proposed algorithm, each iteration extracts ($pDK+MK$) variables, where $pDK$ variables are randomly picked out from $DK$ variables with identical probability and the rest remain unchanged. Owing to the highly sparse codes, the original signals can still be represented by a portion of the codes with a reasonable subsampling rate. Herein, unlike the stochastic Frank-Wolfe algorithm that generally requires more iterations to reach convergence, the convergence of the proposed algorithm will not be significantly affected by the subsampling manipulation. This will be experimentally verified in Section~\ref{sec:result}.

The proposed subsampling strategy will be implemented in the fashions of {\em batch mode} (stochastic batch CSC) and {\em online mode} (stochastic online CSC).

\subsection{Stochastic Batch CSC (SBCSC)}
We first formulate the batch-mode of the proposed method as shown in Algorithm~\ref{algo:SBCSC}, where $N$ is the number of total input images, $\code^i$ is the sparse code associated with $i$-th image, and $p$ is the uniform probability for one code been selected. We choose $p=\{1, 0.5, 0.2, 0.1, 0.05\}$ for testing in this work, where $p=1$ indicates no subsampling, and $p=0.05$ indicates a subsampling rate of $5\%$. Problem~\ref{eq:updatingCode} is the standard LASSO, which can be solved by plenty of optimization frameworks. We found that solving it by ADMM delivers a good balance for computation time and convergence within a moderate number of iterations. Specifically, the data fitting term and the $L_1$ penalty term are split, forming two separate substeps. The first substep is a quadratic programming (QP) problem, and we can either cache the matrix factorization by Cholesky decomposition (when $N$ is relatively large), or solve it by Conjugate Gradient (when $N$ is relatively small). The second substep can be solved by a point-wise shrinkage operation. Problem~\ref{eq:updatingFilter} is a quadratic constrained quadratic programming (QCQP) problem, and it can be efficiently solved by projected block coordinate decent. Empirically, a single iteration is enough with $\filter$ computed in previous iteration as a warm start. We set the hyperparameters $\lambda=1$, the ADMM iteration fixed to 10, the augmented Lagrangian penalty $\rho$ to $10 \lambda$, and the over-relaxation strategy within ADMM is applied with $\alpha = 1.8$. For a detailed description of the above two solvers, please refer to the supplement.

Every outer loop of SBCSC involves all of the training images, thus it would be computationally expensive to process them simultaneously and also runs out of memory quickly with the increase of the number of images. The batch-based learning algorithm lacks of the capability to scale up to very large datasets or to handle dynamically changed training data.

\begin{algorithm}[H]
\caption{SBCSC} \label{algo:SBCSC}
\begin{algorithmic}[1]
\State $\text{Initialize } \filter, ~p$
\While {not converge}
    \State $ \text{Randomly sample }\code \text{ with rate } p $
    \For{i=1 to N}
        \State $ \text{Update } \code^i \text{ by solving problem~(\ref{eq:updatingCode}})$
    \EndFor
    \State $\text{Update } \filter \text{ by solving problem~(\ref{eq:updatingFilter}})$
\EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{Stochastic Online CSC (SOCSC)}
We can further tackle the proposed problem in the online fashion for a gain of scalability. In the online learning setting as shown in Algorithm~\ref{algo:SOCSC}, each iteration only draws one or a subset (mini-batch) of the total training images, hence the complexity per loop is independent of the training sample size. Then, given the sampled image $\signal^t$ at $t$-th iteration, we can compute the corresponding subsampled sparse codes $\code^t$ by
\begin{equation} \label{eq:updatingCodeOnline}
    \code^t = \minimize{\code} \frac{1}{2}\|\signal^t - (\Filter^{t-1} (\mask^t)^\top)(\mask^t \code) \|_2^2 + \lambda \|\mask^t \code\|_1,
\end{equation}
where $\Filter^{t-1}$ is constructed from the dictionary learned by $(t-1)$th iteration, and $\mask^t$ is the subsample matrix at current iteration. After obtaining the sparse codes, the dictionary is updated by:
\begin{equation}
\begin{split}
    &\filter^t = \minimize{\filter} \frac{1}{2t}\sum_{i=1}^{t} \|\signal^i - \Code^i \filter \|_2^2 \\
    &\text{subject to} \quad \|\filter_k\|^2_2 \leq 1 ~ \forall k \in \{1,\dots,K\}.
\end{split}
\end{equation}
Notice that updating dictionary involves all of the past training images and sparse codes. As shown in~\cite{mairal2009online,mairal2010online}, we can get rid of explicitly storing those data by introducing two surrogate matrices $\surC \in \mathbb{R}^{KM \times KM}$ and $\surB \in \mathbb{R}^{KM \times 1}$, which carry all of the required information for updating $\filter$, and can be iteratively updated by:
\begin{equation} \label{eq:updateSur}
\begin{split}
    \surC^t  = \frac{t-1}{t} \surC^{t-1} + \frac{1}{t}(\Code^t)^\top \Code^t \\
    \surB^t  = \frac{t-1}{t} \surB^{t-1} + \frac{1}{t}(\Code^t)^\top \signal^t
\end{split}
\end{equation}
If so, the updated dictionary can be obtained by solving:
\begin{equation} \label{eq:updatingFilterOnline}
\begin{split}
    & \filter^t = \minimize{\filter} \frac{1}{2} \filter^\top \surC^t\filter - \filter^\top \surB^t \\& \text{subject to} \quad \|\filter_k\|_2^2 \leq 1 ~ \forall k \in \{1,\dots,K\}.
\end{split}
\end{equation}
Problem~\ref{eq:updatingCodeOnline} and problem~\ref{eq:updatingFilterOnline} are solved in the same way as that for SBCSC.

\begin{algorithm}[H]
\caption{SOCSC} \label{algo:SOCSC}
\begin{algorithmic}[1]
\State $\text{Initialize} ~ t=0, ~\filter^t, ~p, ~\surC^t = 0, ~\surB^t = 0$
\While {not converge}
    \State $t \gets t+1$
    \State $ \text{draw } \signal^t \text{ from training images} $
    \State $ \text{Randomly sample }\code^t \text{ with rate } p $
    \State $ \text{Compute } \code^t \text{ by solving problem~(\ref{eq:updatingCodeOnline}) using } \filter^{t-1}$
    \State Compute $\surC^t$ and $\surB^t$ by eq~(\ref{eq:updateSur})
    \State Compute $\filter^t$ by solving problem~(\ref{eq:updatingFilterOnline})
\EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{Complexity Analysis}
Recall that $D$ is the number of pixels for a single image, $K$ is the number of filters, and $M$ is the size of the filter support. For the state-of-the-art frequency-domain solvers, it has the time complexity $\mathcal{O}(K^2D + KDlog(D))$ for a single data pass. 

Compared with SBCSC, updating $\code$ (suppose we solve it by Conjugate Gradient) has the time complexity of $\mathcal{O}(pKMD \sqrt{\tau})$ where $pKMD$ is the number of non-zero elements in $(\Filter \mask^\top)$ and $\tau$ is the condition number of $(A^\top A + \rho I)$ where $A = \Filter \mask^\top$. With a reasonable selection of the subsampling rate, this time complexity is comparable with that of frequency-domain solvers. As for updating $\filter$, it takes $\mathcal{O}(K^2M^2)$ time. This is comparable to $\mathcal{O}(K^2D)$ in common CSC settings ($M \ll D$), while it requires multiple iterations in frequency domain and only one pass required by the proposed method, which greatly saves the time. Overall, the proposed method has the time complexity of $\mathcal{O}(pKMD \sqrt{\tau} + K^2M^2)$.

The time complexity of SOCSC is similar to that of SBCSC, apart from two additional steps to update the surrogate matrices. Updating $\surC$ and $\surB$ involves computing $\Code^\top\Code$ and $\Code^\top x$. Though $\Code$ has the dimensions of $D \times KM$, it is a highly sparse matrix with only $\mathcal{O}(D)$ non-zero elements. Therefore, the total time complexity will not be obviously affected. 