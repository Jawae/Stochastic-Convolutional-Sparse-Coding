\section{Stochastic Convolutional Sparse Coding}
\subsection{The Model}
\note{The previous description was not accurate: Eq.2 is not a variant
  of the CSC model, and does not replace Eq.1! Eq.2 is supposed to be
  used inside an outer loop to implement Eq.1. This outer loop is an
  implementation of CSC and Eg.1.  In order to really explain this
  properly, we need to introduce an outer loop parameter for both
  $\mask$ and $\mathcal{M}$.}

Based on the strong sparsity of the codes $\code$, we propose to
implement the CSC learning problem~\eqref{eq:CSCmodel} iteratively,
where at $\ite$-th iteration we only consider a random subset
$\mathcal{M}_k^{\ite}(\code_k)$ of the codes:
\begin{align}
    &\mini{\filter,\code}  \frac{1}{2}\|\signal - \sum_{k=1}^{K} \filter_k * \mathcal{M}_k^{\ite}(\code_k) \|_2^2 + \lambda \sum_{k=1}^{K}\| \mathcal{M}_k^{\ite}(\code_k) \|_1  \nonumber \\
    &\text{subject to} ~ \|\filter_k\|^2_2 \leq 1 ~~ \forall k \in \{1,\dots,K\}.
\end{align}
\todo{add outer loop variable}
\todo{There are some notational issues here -- convolution with an
  irregularly subsampled signal! Lets discuss on Sunday.}
Here $\mathcal{M}_k^{\ite}(\code_k)$ is the operation to subsample the
corresponding code $\code_k$ following a Bernoulli distribution with
probability $p$, drawn at current iteration.
%The probability value $p$ controls the percentage of
%the sparse codes that will be considered by the solver.
In the case of $p=1$, the proposed model is identical with the
classical CSC model. When $p<1$, the algorithm only solves a subset of
the sparse codes at chosen positions in each iteration, and
accordingly, the update of dictionary $d$ is based on the selected
portion of the sparse codes. Similar to solving the classical CSC
problem, we can apply a coordinate descent algorithm, alternating on
subproblems of $\mathcal{M}(\code)$ and $\filter$, to solve the above
bi-convex optimization problem. Specifically, the modified
minimization problem for $\code$ can be formulated as:
\begin{equation} \label{eq:updatingCode}
    \code^{\ite} = \minimize{\code} \frac{1}{2}\|\signal - (\Filter^{\ite-1} (\mask^{\ite})^\top) (\mask^{\ite} \code) \|_2^2 + \lambda \|\mask^{\ite} \code\|_1,
\end{equation}
\todo{add outer loop variable}
where $\Filter^{\ite-1} = [\Filter_1^{\ite-1}, \dots, \Filter_K^{\ite-1}] \in \mathbb{R}^{D
  \times DK}$, which is constructed from the dictionary learned by
$(\ite-1)$-th iteration, $\code = [\code_1, \dots, \code_K] \in
\mathbb{R}^{DK}$. The convolution operators are expressed as a matrix
multiplication so that $ \Filter \code = \sum_{k=1}^{K} \filter_k *
\code_k$. Therefore, each part of $\Filter$ is a Toeplitz
matrix. Matrix $\mask^{\ite}$ is a $\mathbb{R}^{pDk \times DK}$ random
diagonal matrix with $1$ for the entries corresponding to the sampled
sparse codes and $0$ elsewhere, performing the function of
$\mathcal{M}^{\ite}(\code)$. Matrix $\mask^{\ite}$ will be redrawn at each
iteration. It is worth stressing that $(\Filter^{\ite-1} (\mask^{\ite})^\top)$ is not
performing a subsampling operation on the dictionary, \note{not sure
  what this is supposed to say:} which aims to remove the columns in $\Filter^{\ite}$ those are filtered out
by $\mathcal{M}^{\ite}(\code)$.
\note{See notation issue above; we need to discuss this next week:}
Due to the introduced subsampling matrix, the
convolution operator cannot be implemented in the Fourier
domain. However, due to the subsampling
strategy, the number of variables that need to be computed for this
subproblem is $pDK$ instead of $DK$, which leads to a reduction
of the spatial-domain computation time by a factor of $\frac{1}{p}$.

After computing the subsampled sparse codes $\mathcal{M}^{\ite}(\code)$, we can then project them
onto the original spatial support by substituting $0$ for the
masked-out values, obtaining $z^{\ite}$. Afterwards, the dictionary can be updated by:
\begin{equation} \label{eq:updatingFilter}
\begin{split}
   & \filter^{\ite} = \minimize{\filter} \frac{1}{2}\|\signal - \Code^{\ite} \filter \|_2^2 \\
   & \text{subject to}  ~ \|\filter_k\|^2_2 \leq 1 ~ \forall k \in \{1,\dots,K\},
\end{split}
\end{equation}
\todo{add outer loop variable} where $\Code^{\ite}= [\Code_1^{\ite}, \dots, \Code_K^{\ite}]
\in \mathbb{R}^{D \times MK}$ is a concatenation of Toeplitz matrices,
and $\Code_k^{\ite}$ is constructed from the associated $\code_k^{\ite}$, $\filter=
[\filter_1,\dots,\filter_K] \in \mathbb{R}^{MK}$ such that $ \Code
\filter = \sum_{k=1}^{K} \filter_k * \code_k$. In typical CSC
settings, $M \ll D$, hence there is no need to perform subsampling on
the dictionary.

\note{I have no idea what the next two sentences are supposed to say:}
The work of~\cite{mensch2016dictionary} also applies a so-called
stochastic subsampling strategy for dictionary learning, while the
subsampling is randomly performed on the observed signals, differing
from that of our proposed. In addition, it is basically applied for
factorizing matrix that is large in both dimensions. Although CSC can
be expressed in a form similar to matrix factorization, the two
problems are in practice quite different due to the convolutional
nature of the CSC model.  One can think of our proposed randomization
approach as a variant of the stochastic Frank-Wolfe
algorithm~\cite{reddi2016stochastic,pmlr-v80-kerdreux18a}, in which
the optimization is performed on a subset of the variables which are
randomly extracted based on a certain probability distribution at each
iteration. For the proposed algorithm, each iteration extracts
($pDK+MK$) variables, where $pDK$ variables are randomly picked from a
total $DK$ variables, and the rest remain unchanged. Owing to the fact
that the codes are highly sparse, the original signals can still be
represented by a portion of the codes with a reasonable subsampling
rate. Therefore the convergence of the proposed algorithm will not be
significantly affected by the subsampling manipulation, unlike the
general case of the stochastic Frank-Wolfe algorithm, which usually
requires more iterations to reach convergence. This insight is
experimentally verified in Section~\ref{sec:result}.

In the following we introduce two different outer loop structures to
utilize the proposed subsampling strategy. First, we introduce a {\em
  batch mode} method (stochastic batch CSC) that learns from all
images simultaneously, and second we introduce an {\em online}
variant (stochastic online CSC).

\subsection{Stochastic Batch CSC (SBCSC)}
We first introduce a batch-mode version of the proposed method as
shown in Algorithm~\ref{algo:SBCSC}, where $N$ is the number of total
input images, $(\code^i)^{\ite}$ is the sparse code associated with $i$-th
image at $\ite$-th iteration, and $p$ is the uniform probability for one code been
selected. We choose $p=\{1, 0.5, 0.2, 0.1, 0.05\}$ for testing in this
work, where $p=1$ indicates no subsampling, and $p=0.05$ indicates a
subsampling rate of $5\%$.

\begin{algorithm}[H]
\caption{SBCSC} \label{algo:SBCSC}
\begin{algorithmic}[1]
\State $\text{Initialize}  ~ \ite=0, ~\filter^t, ~p$
\While {not converge}
    \State $\ite \gets \ite+1$
    \State $ \text{Randomly sample }\code^{\ite} \text{ with rate } p $
    \For{i=1 to N}
        \State $ \text{Compute } (\code^i)^{\ite} \text{ by solving problem~(\ref{eq:updatingCode}})$
    \EndFor
    \State $\text{Compute } \filter^{\ite} \text{ by solving problem~(\ref{eq:updatingFilter}})$
\EndWhile
\end{algorithmic}
\end{algorithm}


Problem~\eqref{eq:updatingCode} is the standard LASSO, which can be
solved by plenty of optimization frameworks. We found that solving it
with ADMM delivers a good balance between computation time and
convergence within a moderate number of iterations. Specifically, the
data fitting term and the $L_1$ penalty term are split, forming two
separate substeps. The first substep is a quadratic programming (QP)
problem, and we can either cache the matrix factorization by Cholesky
decomposition (when $N$ is relatively large), or solve it iteratively
by Conjugate Gradient (when $N$ is relatively small). The second
substep can be solved by a point-wise shrinkage operation.
%The above two substeps alternatively solved within the ADMM
%iterations.
Problem~\ref{eq:updatingFilter} is a quadratic constrained
quadratic programming (QCQP) problem, and it can be efficiently solved
by projected block coordinate decent. Empirically, a single iteration
is enough with $\filter$ computed in previous iteration as a warm
start. We set the hyperparameters $\lambda=1$, the ADMM iteration
fixed to 10, the augmented Lagrangian penalty $\rho$ to $10 \lambda$,
and the over-relaxation strategy within ADMM is applied with $\alpha =
1.8$. For a detailed description of the above two solvers, please
refer to the supplement.

Every outer loop of SBCSC involves all of the training images, which
makes it computationally expensive to process them all simultaneously.
Furthermore, memory consumption quickly becomes an issue with
increasing numbers of images. Thus, the batch-based learning algorithm
lacks the ability to scale up to very large datasets or to handle
dynamically changing training data.


\subsection{Stochastic Online CSC (SOCSC)}

\begin{algorithm}[H]
\caption{SOCSC} \label{algo:SOCSC}
\begin{algorithmic}[1]
\State $\text{Initialize} ~ t=0, ~\filter^t, ~p, ~\surC^t = 0, ~\surB^t = 0$
\While {not converge}
    \State $t \gets t+1$
    \State $ \text{draw } \signal^t \text{ from training images} $
    \State $ \text{Randomly sample }\code^t \text{ with rate } p $
    \State $ \text{Compute } \code^t \text{ by solving problem~(\ref{eq:updatingCodeOnline}) using } \filter^{t-1}$
    \State Compute $\surC^t$ and $\surB^t$ by Eq.~\ref{eq:updateSur}
    \State Compute $\filter^t$ by solving problem~(\ref{eq:updatingFilterOnline})
\EndWhile
\end{algorithmic}
\todo{Find the best place for this algorithm (probably higher up)}
\end{algorithm}

In order to address this scalability issue, we can tackle the
Stochastic CSC problem in an online fashion. In the online learning
setting as shown in Algorithm~\ref{algo:SOCSC}, each iteration only
draws one or a subset (mini-batch) of the total training images, hence
the complexity per loop is independent of the training sample
size. Then, given the sampled image $\signal^t$ at $t$-th iteration,
we can compute the corresponding subsampled sparse codes $\code^t$ by
\begin{equation} \label{eq:updatingCodeOnline}
    \code^t = \minimize{\code} \frac{1}{2}\|\signal^t - (\Filter^{t-1} (\mask^t)^\top)(\mask^t \code) \|_2^2 + \lambda \|\mask^t \code\|_1.
\end{equation}
\note{Here we have $t$ as an iteration variable now -- I don't think
  this is a good choice ($t$ is usually reserved for continuous
  values, better would be $i$ or $j$, and we need to introduce these
  earlier in the paper.}
The only difference between Eq. ~\ref{eq:updatingCode} is that $\signal^t$ only contains a portion of the total images. After obtaining
the sparse codes, the dictionary is updated by:
\begin{equation}
\begin{split}
    &\filter^t = \minimize{\filter} \frac{1}{2t}\sum_{i=1}^{t} \|\signal^i - \Code^i \filter \|_2^2 \\
    &\text{subject to} \quad \|\filter_k\|^2_2 \leq 1 ~ \forall k \in \{1,\dots,K\}.
\end{split}
\end{equation}
Note that updating the dictionary in this fashion involves all of the
past training images and sparse codes. Using techniques developed for
regular (non-convolutional) dictionary
learning~\cite{mairal2009online,mairal2010online}, we can get rid of
explicitly storing this data by introducing two surrogate matrices
$\surC \in \mathbb{R}^{KM \times KM}$ and $\surB \in \mathbb{R}^{KM
  \times 1}$. These carry all of the required information for updating
$\filter$, and can be iteratively updated by:
\begin{equation} \label{eq:updateSur}
\begin{split}
    \surC^t  = \frac{t-1}{t} \surC^{t-1} + \frac{1}{t}(\Code^t)^\top \Code^t \\
    \surB^t  = \frac{t-1}{t} \surB^{t-1} + \frac{1}{t}(\Code^t)^\top \signal^t
\end{split}
\end{equation}
With these surrogate matrices, the updated dictionary can be obtained
by solving
\begin{equation} \label{eq:updatingFilterOnline}
\begin{split}
    & \filter^t = \minimize{\filter} \frac{1}{2} \filter^\top \surC^t\filter - \filter^\top \surB^t \\& \text{subject to} \quad \|\filter_k\|_2^2 \leq 1 ~ \forall k \in \{1,\dots,K\}.
\end{split}
\end{equation}
Problem~\eqref{eq:updatingCodeOnline} and
problem~\eqref{eq:updatingFilterOnline} are solved in the same way as
that for SBCSC.

\subsection{Complexity Analysis}
Recall that $D$ is the number of pixels for a single image, $K$ is the
number of filters, and $M$ is the size of the filter
support. Commonly, we can assume $K \approx M$.  State-of-the-art
frequency-domain solvers then have the time complexity
$\mathcal{O}(K^2D + KDlog(D))$ for a single data pass.

The time complexity of updating $\code$ in our SBCSC algorithm using
Conjugate Gradient is $\mathcal{O}(pKMD \sqrt{\tau})$, where $pKMD$ is
the number of non-zero elements in $(\Filter \mask^\top)$ and $\tau$
is the condition number of $(\matA^\top \matA + \rho I)$ where $\matA
= \Filter \mask^\top$. With a reasonable selection of the subsampling
rate, this time complexity is comparable to that of frequency-domain
solvers.

Updating the filters $\filter$ takes $\mathcal{O}(K^2M^2)$ time. This
is comparable to $\mathcal{O}(K^2D)$ in the common CSC setting ($M \ll
D$). However, multiple ADMM iterations are required in the frequency
domain to compute $\filter$ while only a single pass is required by
the proposed method, which greatly reduces the computation
time. Overall, the proposed method has the time complexity of
$\mathcal{O}(pKMD \sqrt{\tau} + K^2M^2)$.

The time complexity of SOCSC is similar to that of SBCSC, apart from
two additional steps to update the surrogate matrices. Updating
$\surC$ and $\surB$ involves computing $\Code^\top\Code$ and
$\Code^\top x$. Although $\Code$ has dimensions of $D \times KM$, it
is a highly sparse matrix with only $\mathcal{O}(D)$ non-zero
elements. Therefore, the total performance is not affected
significantly.


% --- DO NOT DELETE ---
% Local Variables:
% mode: latex
% mode: flyspell
% mode: TeX-PDF
% End:

