\section{Stochastic Convolutional Sparse Coding}
\subsection{The Model}
Since the images are represented by a set of highly sparse codes, we have observed that they can be well reconstructed by a random subset of the sparse codes. Based on this observation, we propose a variant of the $\code$-subproblem using subsampling strategy, and formulate the following modified minimization problem:
\begin{equation} \label{eq:updatingCode}
    \minimize{\code} \frac{1}{2}\|\signal - (\Filter \mask^T)(\mask \code) \|_2^2 + \lambda \|\mask \code\|_1,
\end{equation}
where $\Filter = [\Filter_1 ... \Filter_K] \in \mathbb{R}^{D \times DK}$, $\code = [\code_1^T ... \code_K^T]^T \in \mathbb{R}^{DK \times 1}$, the convolution operator are formulated in matrix multiplication so that $ \Filter \code = \sum_{k=1}^{K} \filter_k * \code_k$.  $\mask$ is a randomized sub-sampling diagonal matrix with $1$ on the entries corresponding to the sampled sparse codes, performing the function of choosing a random portion of the sparse codes for reconstruction. Herein, the $\code$-subproblem only computes the sparse codes in the chosen positions. Due to the introduced subsampling matrix, the convolution operator will not hold, hence it cannot be solved in the frequency domain. This subsampling strategy is akin to the dropout technique~\cite{srivastava2014dropout}, which has been widely used to prevent overfitting when training deep neural networks. However, different from dropout that generally needs more time to train a neural network than standard approaches, with a good selection of subsampling probability $p$, the proposed method shows less per iteration running time and faster convergence than state-of-the-arts (see section~\ref{sec:result}).

After obtaining the subsampled codes, we can then project them onto its original spatial support with $0$ filling. Afterwards, the dictionary can be updated by:
\begin{equation} \label{eq:updatingFilter}
    \minimize{\filter} \frac{1}{2}\|\signal - \Code \filter \|_2^2, ~\subjectto ~ \|\filter_k\|^2_2 \leq 1 ~ \forall k \in {1,...K},
\end{equation}
where $\Code= [\Code_1 ... \Code_K] \in \mathbb{R}^{D \times MK}$ and $\filter= [\filter_1^T ... \filter_K^T]^T \in \mathbb{R}^{MK \times 1}$ such that $ \Code \filter = \sum_{k=1}^{K} \filter_k * \code_k$.

Like the approach in~\cite{heide2015fast}, we interleave on the above two subproblems to tackle the bi-convex problem, and each of them will be solved by ADMM. The jointly optimization framework will be implemented in the fashions of batch mode and online mode.

\begin{minipage}[t]{0.5\textwidth}
\vspace{0pt}
\begin{algorithm}[H]
\caption{SBCSC} \label{algo:SBCSC}
\begin{algorithmic}[1]
\State $\text{Initialize} ~ \filter;$
\While {not converge}
    \State $\Filter \gets \text{constructD}(\filter);$
    \For{i=1 to N}
        \State $ \text{Generate } \mask \text{ given } p;$
        \State $ \text{Update } \code^i \text{ by solving problem~\ref{eq:updatingCode}};$
    \EndFor
    \State $\Code \gets \text{constructZ}(\code^{1,...,N});$
    \State $\text{Update } \filter \text{ by solving problem~\ref{eq:updatingFilter}} ;$
\EndWhile
\end{algorithmic}
\end{algorithm}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\vspace{0pt}
\begin{algorithm}[H]
\caption{SOCSC} \label{algo:SOCSC}
\begin{algorithmic}[1]
\State $\text{Initialize} ~ t=0, \filter^t,  \surC^t = 0, \surB^t = 0$
\While {not converge}
    \State $\Filter \gets \text{constructD}(\filter^t);$
    \State $t \gets t+1$
    \State draw($\signal^t$, $\mask$);
    \State $ \text{Compute } \code^t \text{ by solving problem~\ref{eq:updatingCodeOnline}};$
    \State $\Code \gets \text{constructZ}(\code^t)$
    \State Compute $\surC^t$ and $\surB^t$ by eq~\ref{eq:updateSur}
    \State Update $\filter^t$ by solving problem~\ref{eq:updatingFilterOnline}
\EndWhile
\end{algorithmic}
\end{algorithm}
\end{minipage}

\subsection{Stochastic Batch CSC (SBCSC)}
We first formulate the batch-mode of the proposed method as shown in Algorithm~\ref{algo:SBCSC}, where $N$ is the number of total input images, $\code^i$ is the sparse code associated with $i$-th image, and $p$ is the probability for one specific code been selected. Every outer loop involves all of the training images, thus it would be expensive in computing these and also runs out of memory quickly with the increase of the number of images. The batch-based learning algorithm lacks of the capability to scale up to very large datasets or to handle dynamically changed training data.

\subsection{Stochastic Online CSC (SOCSC)}
We can further tackle the proposed problem in online fashion for a gain of scalability. In the online learning setting, each iteration only draws one or a subset (minibatch) of the total training images, hence the complexity per loop is independent of the training sample size. Then, given the sampled image $\signal^t$ at $t$-th iteration, we can compute the corresponding sparse codes $\code^t$ by
\begin{equation} \label{eq:updatingCodeOnline}
    \code^t = \minimize{\code} \frac{1}{2}\|\signal^t - (\Filter^{t-1} \mask^T)(\mask \code) \|_2^2 + \lambda \|\mask \code\|_1,
\end{equation}
where $\Filter^{t-1}$ is constructed from the dictionaries learned by $(t-1)$th iteration. After obtaining the sparse codes, the dictionary is updated by:
\begin{equation}
    \filter^t = \minimize{\filter} \frac{1}{2t}\sum_{i=1}^{t} \|\signal^i - \Code^i \filter \|_2^2, ~\subjectto ~ \|\filter_k\|^2_2 \leq 1 ~ \forall k \in {1,...K}.
\end{equation}
Notice that updating dictionary involves all the past training images and sparse codes. As shown in~\cite{mairal2009online,mairal2010online}, we can get rid of explicitly storing those data by introducing two surrogate matrices $\surC \in \mathbb{R}^{KM \times KM}$ and $\surB \in \mathbb{R}^{KM \times 1}$, which carry all of the required information for updating $\filter$, and can be iteratively updated by:
\begin{equation} \label{eq:updateSur}
    \surC^t  = \frac{t-1}{t} \surC^{t-1} + \frac{1}{t}(\Code^t)^T\Code^t ~~~~~~~~~ \surC^t  = \frac{t-1}{t} \surB^{t-1} + \frac{1}{t}(\Code^t)^T\signal^t
\end{equation}
If so, the updated dictionary can be obtained by solving:
\begin{equation} \label{eq:updatingFilterOnline}
    \filter^t = \minimize{\filter} \frac{1}{2} \filter^T\surC\filter - \filter^T\surB, ~ \subjectto ~ \|\filter_k\|_2^2 \leq 1 ~ \forall k \in {1,...K}
\end{equation}

\subsection{Implementation Details}
Updating $\code$ in Algorithm\ref{algo:SBCSC} (Problem~\ref{eq:updatingCode}) is the standard LASSO, and we solve it by well-known ADMM framework. The data fitting term and the $L_1$ penalty term are split, forming $x$-minimization step and $z$-minimization step, respectively. $x$-minimization step is a quadratic programming (QP) problem, and we cache the matrix factorization by Cholesky decomposition. $z$-minimization step can be solved by a point-wise shrinkage operation. Accordingly, updating $\filter$ (problem~\ref{eq:updatingFilter}) is a quadratic constrained quadratic programming (QCQP) problem, and we solve it using similar splitting and optimization strategies as solving Problem~\ref{eq:updatingCode}, except using Conjugate Gradient (CG) for $x$-minimization step and replacing the shrinkage function by projecting onto the constrained set for $z$-minimization step. The $\filter$-subproblem is warm started with previously computed results and these two subproblems are interleaved on their auxiliary variables. The corresponding subproblems in Algorithm~\ref{algo:SOCSC} are solved following the same optimization strategy as discussed above. We set the hyperparameters $\lambda=1$,  the augmented Lagrangian penalty $\rho_{\code}$ and $\rho_{\filter}$ to $10 \lambda$ and $50 \lambda$, respectively, for $\code$- and $\filter$-subroblem. The ADMM iteration is fixed to 10 for all subroutines, and the over-relaxation strategy within ADMM is applied with $\alpha = 1.8$. The residual tolerance for CG is $1e^{-5}$.

All the training and evaluation processes in this manuscript are performed on contrast normalized images. %We also tried to learning filters from images without pre-processing with little parameters tuning, which successfully learns similar Gabor-like features as learned from contrast normalized images, as well as low frequency filters. The results regarding to this will not be discussed in this work, while it can be applied to other applications where low frequency parts also need to be taken into account. %
It should be noted that matrix $\Code$ needs to be explicitly constructed and it may lead to a considerable computational burden, resulting in a heavy overhead if processed in an inefficient way. In our implementation, the vector-to-matrix mapping information is cached for an efficient indexing. As for matrix $\Filter$, we only explicitly construct matrix $(\Filter \mask^T)$ rather than $\Filter$ and $\mask$ separately for the sake of efficiency, and the similar indexing strategy is applied.