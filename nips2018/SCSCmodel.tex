\section{Stochastic Convolutional Sparse Coding}
\subsection{The Model}
Since the images are represented by a set of highly sparse codes, we have observed that they can be well reconstructed by a random subset of the sparse codes. Based on this observation, we propose a variant of the $\code$-subproblem using subsampling strategy, and formulate the following modified minimization problem:
\begin{equation} \label{eq:updatingCode}
    \minimize{\code} \frac{1}{2}\|\signal - (\Filter \mask^\top)(\mask \code) \|_2^2 + \lambda \|\mask \code\|_1,
\end{equation}
where $\Filter = [\Filter_1, \dots, \Filter_K] \in \mathbb{R}^{D \times DK}$, $\code = [\code_1, \dots, \code_K] \in \mathbb{R}^{DK}$, the convolution operator are formulated in matrix multiplication so that $ \Filter \code = \sum_{k=1}^{K} \filter_k * \code_k$.  Matrix $\mask$ is a random diagonal subsampling matrix with ones on the entries corresponding to the sampled sparse codes and zeros elsewhere, performing the function of choosing a random portion of the sparse codes for reconstruction. Herein, the $\code$-subproblem only computes the sparse codes in the chosen positions. Due to the introduced subsampling matrix, the convolution operator will not hold, hence it cannot be solved in the frequency domain. This stochastic subsampling strategy is akin to the dropout technique~\cite{srivastava2014dropout}, which has been widely used to prevent overfitting when training deep neural networks. However, different from dropout that generally needs more time to train a neural network than standard approaches, with a good selection of subsampling probability $p$, the proposed method shows less per iteration running time and faster convergence than state-of-the-arts (see Section~\ref{sec:result}).

After obtaining the subsampled sparse codes, we can then project them onto its original spatial support with $0$ filling. Afterwards, the dictionary can be updated by:
\begin{equation} \label{eq:updatingFilter}
    \minimize{\filter} \frac{1}{2}\|\signal - \Code \filter \|_2^2 \quad \text{subject to} \quad \|\filter_k\|^2_2 \leq 1 ~ \forall k \in \{1,\dots,K\},
\end{equation}
where $\Code= [\Code_1, \dots, \Code_K] \in \mathbb{R}^{D \times MK}$ is a concatenated matrix, where $\Code_k$ is constructed from the associated $\code_k$, $\filter= [\filter_1,\dots,\filter_K] \in \mathbb{R}^{MK}$ such that $ \Code \filter = \sum_{k=1}^{K} \filter_k * \code_k$.

Similar to the approach in~\cite{heide2015fast}, we alternate the above two subproblems to tackle the bi-convex problem, and each of them will be solved by ADMM. The joint optimization framework will be implemented in the fashions of {\em batch mode} (stochastic batch CSC) and {\em online mode} (stochastic online CSC).

\begin{minipage}[t]{0.5\textwidth}
\vspace{0pt}
\begin{algorithm}[H]
\caption{SBCSC} \label{algo:SBCSC}
\begin{algorithmic}[1]
\State $\text{Initialize} ~ \filter, p$
\While {not converge}
    \State $ \text{Generate } \mask $
    \State $\Filter \gets \text{constructD}(\filter)$
    \For{i=1 to N}
        \State $ \text{Update } \code^i \text{ by solving problem~(\ref{eq:updatingCode}})$
    \EndFor
    \State $\Code \gets \text{constructZ}(\code^{1,\dots,N})$
    \State $\text{Update } \filter \text{ by solving problem~(\ref{eq:updatingFilter}})$
\EndWhile
\end{algorithmic}
\end{algorithm}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\vspace{0pt}
\begin{algorithm}[H]
\caption{SOCSC} \label{algo:SOCSC}
\begin{algorithmic}[1]
\State $\text{Initialize} ~ t=0, \filter^t,  \surC^t = 0, \surB^t = 0$
\While {not converge}
    \State $t \gets t+1$
    \State draw($\signal^t$, $\mask$)
    \State $\Filter^{t-1} \gets \text{constructD}(\filter^{t-1})$
    \State $ \text{Compute } \code^t \text{ by solving problem~(\ref{eq:updatingCodeOnline})}$
    \State $\Code^t \gets \text{constructZ}(\code^t)$
    \State Compute $\surC^t$ and $\surB^t$ by eq~(\ref{eq:updateSur})
    \State Compute $\filter^t$ by solving problem~(\ref{eq:updatingFilterOnline})
\EndWhile
\end{algorithmic}
\end{algorithm}
\end{minipage}

\subsection{Stochastic Batch CSC (SBCSC)}
We first formulate the batch-mode of the proposed method as shown in Algorithm~\ref{algo:SBCSC}, where $N$ is the number of total input images, $\code^i$ is the sparse code associated with $i$-th image, and $\code^{1,\dots,N}$ denotes the stack of all the sparse codes, $p$ is the uniform probability for one code been selected. We choose $p=\{1, 0.5, 0.2, 0.1, 0.05\}$ for testing in this work, where $p=1$ indicates no subsampling, and $p=0.05$ indicates a subsampling rate of $5\%$. The matrix $\Code$ needs to be explicitly constructed and it may lead to a considerable computational burden, resulting in a heavy overhead if processed in an inefficient way. In our implementation, the vector-to-matrix mapping information is cached for an efficient indexing. As for matrix $\Filter$, we only explicitly construct matrix $(\Filter \mask^\top)$ rather than $\Filter$ and $\mask$ separately for the sake of efficiency, and the similar indexing strategy is applied (line 3 and 4 are practically merged by a single operation, but for an intuitive understanding of the algorithm, we split them in the pseudo code). Every outer loop involves all of the training images, thus it would be computationally expensive to process them simultaneously and also runs out of memory quickly with the increase of the number of images. The batch-based learning algorithm lacks of the capability to scale up to very large datasets or to handle dynamically changed training data.

\subsection{Stochastic Online CSC (SOCSC)}
We can further tackle the proposed problem in the online fashion for a gain of scalability. In the online learning setting as shown in Algorithm~\ref{algo:SOCSC}, each iteration only draws one or a subset (mini-batch) of the total training images and the corresponding subsampling diagonal matrix $\mask$, denoted as draw($\signal^t$, $\mask$), hence the complexity per loop is independent of the training sample size. Then, given the sampled image $\signal^t$ at $t$-th iteration, we can compute the corresponding sparse codes $\code^t$ by
\begin{equation} \label{eq:updatingCodeOnline}
    \code^t = \minimize{\code} \frac{1}{2}\|\signal^t - (\Filter^{t-1} \mask^\top)(\mask \code) \|_2^2 + \lambda \|\mask \code\|_1,
\end{equation}
where $\Filter^{t-1}$ is constructed from the dictionary learned by $(t-1)$th iteration. After obtaining the sparse codes, the dictionary is updated by:
\begin{equation}
    \filter^t = \minimize{\filter} \frac{1}{2t}\sum_{i=1}^{t} \|\signal^i - \Code^i \filter \|_2^2 \quad \text{subject to} \quad \|\filter_k\|^2_2 \leq 1 ~ \forall k \in \{1,\dots,K\}.
\end{equation}
Notice that updating dictionary involves all of the past training images and sparse codes. As shown in~\cite{mairal2009online,mairal2010online}, we can get rid of explicitly storing those data by introducing two surrogate matrices $\surC \in \mathbb{R}^{KM \times KM}$ and $\surB \in \mathbb{R}^{KM \times 1}$, which carry all of the required information for updating $\filter$, and can be iteratively updated by:
\begin{equation} \label{eq:updateSur}
    \surC^t  = \frac{t-1}{t} \surC^{t-1} + \frac{1}{t}(\Code^t)^\top \Code^t ~~~~~~~~~ \surB^t  = \frac{t-1}{t} \surB^{t-1} + \frac{1}{t}(\Code^t)^\top \signal^t
\end{equation}
If so, the updated dictionary can be obtained by solving:
\begin{equation} \label{eq:updatingFilterOnline}
    \filter^t = \minimize{\filter} \frac{1}{2} \filter^\top \surC^t\filter - \filter^\top \surB^t \quad \text{subject to} \quad \|\filter_k\|_2^2 \leq 1 ~ \forall k \in \{1,\dots,K\}.
\end{equation}

\subsection{Implementation Details}
Updating $\code$ in Algorithm\ref{algo:SBCSC} (Problem~\ref{eq:updatingCode}) is the standard LASSO, and we solve it by well-known ADMM framework. The data fitting term and the $L_1$ penalty term are split, forming $x$-minimization step and $z$-minimization step, respectively. The $x$-minimization step is a quadratic programming (QP) problem, and we cache the matrix factorization by Cholesky decomposition. The $z$-minimization step can be solved by a point-wise shrinkage operation. Accordingly, updating $\filter$ (Problem~\ref{eq:updatingFilter}) is a quadratic constrained quadratic programming (QCQP) problem, and we solve it using similar splitting and optimization strategies as solving Problem~\ref{eq:updatingCode}, except using Conjugate Gradient (CG) for $x$-minimization step and replacing the shrinkage function by projecting onto the constrained set for $z$-minimization step. The $\filter$-subproblem is warm started with previously computed results and these two subproblems are interleaved on their auxiliary variables. The corresponding subproblems in Algorithm~\ref{algo:SOCSC} are solved following the same optimization strategy as discussed above, and the only exception is for updating $\code$ (Problem~\ref{eq:updatingCodeOnline}), where Cholesky decomposition or conjugate gradient is alternatively chosen depending on the mini-batch size. We set the hyperparameters $\lambda=1$,  the augmented Lagrangian penalty $\rho_{\code}$ and $\rho_{\filter}$ to $10 \lambda$ and $50 \lambda$, respectively, for $\code$- and $\filter$-subproblem. The ADMM iteration is fixed to 10 for all subroutines, and the over-relaxation strategy within ADMM is applied with $\alpha = 1.8$. The residual tolerance for CG is $1e^{-5}$.