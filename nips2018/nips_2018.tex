\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
\usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
%\usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{wrapfig}
\usepackage{subcaption}

\usepackage{caption}
%\captionsetup[table]{position=bottom}

\usepackage{array}
\usepackage{epsfig}
\usepackage{epstopdf}
\usepackage{amsmath}

% Commenting
\usepackage[colorinlistoftodos,bordercolor=orange,backgroundcolor=orange!20,linecolor=orange,textsize=scriptsize]{todonotes}
\newcommand{\peter}[1]{\todo[inline]{\textbf{Peter: }#1}} 
\newcommand{\yinhui}[1]{\todo[inline]{\textbf{Yu: }#1}} 


\input{macro}



\title{Stochastic Convolutional Sparse Coding}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

% snr onlineVSbatch online400Small

\begin{abstract}
The minimization problems formulated in Convolutional Sparse Coding (CSC) have been solved in frequency domain by modern approaches owing to the improved computing efficiency, while it automatically assumes circular boundary conditions. In this work, we propose a novel stochastic spatial-domain solver where a randomized subsampling strategy is introduced in the subproblem of computing sparse codes. The proposed method experimentally shows a faster convergence rate compared to the state-of-the-art, and with a reasonable selection for the subsampling probability, it also achieves a better runtime efficiency. Moreover, we extend the proposed strategy in conjunction with online learning, scaling the CSC model up to arbitrary sample sizes. This combination leads to a significant speedup over the online CSC algorithm solved in frequency domain. We then learn the over-complete dictionary from thousand of images, which demonstrates a better sparse representation of the natural images on account of more abundant filters.
\end{abstract}

\input{Introduction}

\input{CSCmodel}

\input{SCSCmodel}

\input{Results}

\input{Conclusion}

{\small
\bibliographystyle{ieeetr}
\bibliography{egbib}
}


\clearpage
\appendix
\section{More Experiments}

\peter{Include more experiments here}

\end{document}